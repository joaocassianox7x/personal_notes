

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Bias - Variance Decomposition &mdash; Personal Notes</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=e59714d7" />

  
      <script src="../_static/jquery.js?v=5d32c60e"></script>
      <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../_static/documentation_options.js?v=5929fcd5"></script>
      <script src="../_static/doctools.js?v=9bcbadda"></script>
      <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script>window.MathJax = {"loader": {"load": ["[tex]/boldsymbol"]}, "tex": {"packages": {"[+]": ["boldsymbol"]}}}</script>
      <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Metrics for DS, ML, and AI" href="metrics.html" />
    <link rel="prev" title="Logistic Regression" href="logistic_regression.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            Personal Notes
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Introduction</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../aboutme.html">About Me</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intro.html">Introduction of Material</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Physics and Computational Physics</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../physics/pppm_ewald.html">PPPM vs Ewald (Long-Range Coulomb)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../physics/double_pendulum.html">Double Pendulum via Lagrangian Mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../physics/computational_derivatives.html">Computational Derivatives</a></li>
<li class="toctree-l1"><a class="reference internal" href="../physics/force_fields.html">Force Fields : Water</a></li>
<li class="toctree-l1"><a class="reference internal" href="../physics/lammps.html">Molecular Dynamics with LAMMPS</a></li>
<li class="toctree-l1"><a class="reference internal" href="../physics/harmonic_oscillator.html">Quantum Harmonic Oscillator</a></li>
<li class="toctree-l1"><a class="reference internal" href="../physics/split_operator.html">Split-Operator Method (Quantum Time Propagation)</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Math</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../math/functions.html">Functions and Domains</a></li>
<li class="toctree-l1"><a class="reference internal" href="../math/limits.html">Limits</a></li>
<li class="toctree-l1"><a class="reference internal" href="../math/residue_theorem.html">Residue Theorem</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">DS, ML, and AI</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="python.html">Python for Data Science</a></li>
<li class="toctree-l1"><a class="reference internal" href="linear_regression.html">Linear Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="logistic_regression.html">Logistic Regression</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Bias - Variance Decomposition</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#expected-prediction-error">Expected Prediction Error</a></li>
<li class="toctree-l2"><a class="reference internal" href="#expected-squared-error">Expected Squared Error</a></li>
<li class="toctree-l2"><a class="reference internal" href="#the-fundamental-bias-variance-decomposition">The Fundamental Bias-Variance Decomposition</a></li>
<li class="toctree-l2"><a class="reference internal" href="#expected-test-error-integrated">Expected Test Error (Integrated)</a></li>
<li class="toctree-l2"><a class="reference internal" href="#detailed-analysis-of-bias-and-variance">Detailed Analysis of Bias and Variance</a></li>
<li class="toctree-l2"><a class="reference internal" href="#bias-definition-and-interpretation">Bias: Definition and Interpretation</a></li>
<li class="toctree-l2"><a class="reference internal" href="#variance-definition-and-interpretation">Variance: Definition and Interpretation</a></li>
<li class="toctree-l2"><a class="reference internal" href="#relationship-between-bias-and-variance">Relationship Between Bias and Variance</a></li>
<li class="toctree-l2"><a class="reference internal" href="#bias-variance-in-different-contexts">Bias-Variance in Different Contexts</a></li>
<li class="toctree-l2"><a class="reference internal" href="#linear-regression">Linear Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="#regularized-regression">Regularized Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="#decision-trees-and-ensemble-methods">Decision Trees and Ensemble Methods</a></li>
<li class="toctree-l2"><a class="reference internal" href="#cross-validation-and-bias-variance">Cross-Validation and Bias-Variance</a></li>
<li class="toctree-l2"><a class="reference internal" href="#leave-one-out-cross-validation-loocv">Leave-One-Out Cross-Validation (LOOCV)</a></li>
<li class="toctree-l2"><a class="reference internal" href="#k-fold-cross-validation">K-Fold Cross-Validation</a></li>
<li class="toctree-l2"><a class="reference internal" href="#learning-curves-diagnosing-bias-variance-problems">Learning Curves: Diagnosing Bias-Variance Problems</a></li>
<li class="toctree-l2"><a class="reference internal" href="#model-complexity-examples">Model Complexity Examples</a></li>
<li class="toctree-l2"><a class="reference internal" href="#practical-strategies-to-control-bias-variance">Practical Strategies to Control Bias-Variance</a></li>
<li class="toctree-l2"><a class="reference internal" href="#reducing-bias-model-is-too-simple">Reducing Bias (Model is Too Simple)</a></li>
<li class="toctree-l2"><a class="reference internal" href="#reducing-variance-model-is-too-complex">Reducing Variance (Model is Too Complex)</a></li>
<li class="toctree-l2"><a class="reference internal" href="#the-bayes-error-ultimate-lower-bound">The Bayes Error: Ultimate Lower Bound</a></li>
<li class="toctree-l2"><a class="reference internal" href="#bias-variance-in-classification">Bias-Variance in Classification</a></li>
<li class="toctree-l2"><a class="reference internal" href="#summary-table">Summary Table</a></li>
<li class="toctree-l2"><a class="reference internal" href="#references-and-further-reading">References and Further Reading</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="metrics.html">Metrics for DS, ML, and AI</a></li>
<li class="toctree-l1"><a class="reference internal" href="backpropagation.html">Backpropagation</a></li>
<li class="toctree-l1"><a class="reference internal" href="pca.html">Principal Component Analysis (PCA)</a></li>
<li class="toctree-l1"><a class="reference internal" href="transformers.html">Understanding Transformers</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Personal Notes</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Bias - Variance Decomposition</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/ds_ai/bias_variance.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="bias-variance-decomposition">
<h1>Bias - Variance Decomposition<a class="headerlink" href="#bias-variance-decomposition" title="Link to this heading"></a></h1>
<p>Prediction error can be decomposed into three irreducible components: bias, variance, and noise. Understanding this decomposition is essential for:</p>
<ul class="simple">
<li><p>Diagnosing why a model performs poorly</p></li>
<li><p>Selecting appropriate model complexity</p></li>
<li><p>Understanding overfitting and underfitting</p></li>
<li><p>Designing effective regularization strategies</p></li>
</ul>
<section id="expected-prediction-error">
<h2>Expected Prediction Error<a class="headerlink" href="#expected-prediction-error" title="Link to this heading"></a></h2>
<p>Consider a regression problem where we have:</p>
<div class="math notranslate nohighlight">
\[Y = f(X) + \epsilon\]</div>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(Y\)</span> is the target variable</p></li>
<li><p><span class="math notranslate nohighlight">\(X\)</span> is the input feature vector</p></li>
<li><p><span class="math notranslate nohighlight">\(f(X)\)</span> is the unknown true function</p></li>
<li><p><span class="math notranslate nohighlight">\(\epsilon \sim \mathcal{N}(0, \sigma_\epsilon^2)\)</span> is irreducible noise</p></li>
</ul>
<p><strong>Goal:</strong> Estimate <span class="math notranslate nohighlight">\(f(X)\)</span> with a learned function <span class="math notranslate nohighlight">\(\hat{f}(X)\)</span> using training data <span class="math notranslate nohighlight">\(\mathcal{D}\)</span>.</p>
</section>
<section id="expected-squared-error">
<h2>Expected Squared Error<a class="headerlink" href="#expected-squared-error" title="Link to this heading"></a></h2>
<p>For a test point <span class="math notranslate nohighlight">\((X_0, Y_0)\)</span>, the expected squared error is:</p>
<div class="math notranslate nohighlight">
\[\text{EPE}(X_0) = \mathbb{E}_{Y_0, \mathcal{D}} \left[ (Y_0 - \hat{f}(X_0))^2 \right]\]</div>
<p><strong>Step 1: Expand the squared error</strong></p>
<div class="math notranslate nohighlight">
\[\text{EPE}(X_0) = \mathbb{E}_{Y_0, \mathcal{D}} \left[ Y_0^2 - 2Y_0 \hat{f}(X_0) + \hat{f}(X_0)^2 \right]\]</div>
<p><strong>Step 2: Substitute the true model</strong></p>
<p>Since <span class="math notranslate nohighlight">\(Y_0 = f(X_0) + \epsilon\)</span>:</p>
<div class="math notranslate nohighlight">
\[\text{EPE}(X_0) = \mathbb{E}_{Y_0, \mathcal{D}} \left[ (f(X_0) + \epsilon)^2 - 2(f(X_0) + \epsilon)\hat{f}(X_0) + \hat{f}(X_0)^2 \right]\]</div>
<p><strong>Step 3: Expand and separate terms</strong></p>
<div class="math notranslate nohighlight">
\[\text{EPE}(X_0) = \mathbb{E}[f(X_0)^2] + \mathbb{E}[\epsilon^2] + 2\mathbb{E}[f(X_0)\epsilon]
- 2\mathbb{E}[f(X_0)\hat{f}(X_0)] - 2\mathbb{E}[\epsilon \hat{f}(X_0)] + \mathbb{E}[\hat{f}(X_0)^2]\]</div>
<p><strong>Step 4: Simplify using independence</strong></p>
<p>Since <span class="math notranslate nohighlight">\(\epsilon\)</span> is independent of both <span class="math notranslate nohighlight">\(f(X_0)\)</span> and <span class="math notranslate nohighlight">\(\hat{f}(X_0)\)</span>, and <span class="math notranslate nohighlight">\(\mathbb{E}[\epsilon] = 0\)</span>:</p>
<div class="math notranslate nohighlight">
\[\text{EPE}(X_0) = f(X_0)^2 + \sigma_\epsilon^2 - 2f(X_0)\mathbb{E}_\mathcal{D}[\hat{f}(X_0)] + \mathbb{E}_\mathcal{D}[\hat{f}(X_0)^2]\]</div>
<p><strong>Step 5: Complete the decomposition</strong></p>
<p>Add and subtract <span class="math notranslate nohighlight">\((\mathbb{E}_\mathcal{D}[\hat{f}(X_0)])^2\)</span>:</p>
<div class="math notranslate nohighlight">
\[\text{EPE}(X_0) = \left[ f(X_0) - \mathbb{E}_\mathcal{D}[\hat{f}(X_0)] \right]^2 + \mathbb{E}_\mathcal{D}[\hat{f}(X_0)^2] - (\mathbb{E}_\mathcal{D}[\hat{f}(X_0)])^2 + \sigma_\epsilon^2\]</div>
</section>
<section id="the-fundamental-bias-variance-decomposition">
<h2>The Fundamental Bias-Variance Decomposition<a class="headerlink" href="#the-fundamental-bias-variance-decomposition" title="Link to this heading"></a></h2>
<p>The above derivation yields the fundamental decomposition:</p>
<div class="math notranslate nohighlight">
\[\text{EPE}(X_0) = \underbrace{\left[ f(X_0) - \mathbb{E}_\mathcal{D}[\hat{f}(X_0)] \right]^2}_{\text{Bias}^2} + \underbrace{\mathbb{E}_\mathcal{D}\left[ (\hat{f}(X_0) - \mathbb{E}_\mathcal{D}[\hat{f}(X_0)])^2 \right]}_{\text{Variance}} + \underbrace{\sigma_\epsilon^2}_{\text{Irreducible Error}}\]</div>
<p><strong>Bias Term:</strong></p>
<div class="math notranslate nohighlight">
\[\text{Bias}(X_0) = f(X_0) - \mathbb{E}_\mathcal{D}[\hat{f}(X_0)]\]</div>
<p>Represents the systematic error due to model assumptions. A model with high bias makes strong assumptions about the data (e.g., linear when data is nonlinear).</p>
<p><strong>Variance Term:</strong></p>
<div class="math notranslate nohighlight">
\[\text{Var}(X_0) = \mathbb{E}_\mathcal{D}\left[ (\hat{f}(X_0) - \mathbb{E}_\mathcal{D}[\hat{f}(X_0)])^2 \right]\]</div>
<p>Represents the sensitivity of the model to fluctuations in the training data. High variance means small changes in training data lead to large changes in predictions.</p>
<p><strong>Irreducible Error (Noise):</strong></p>
<div class="math notranslate nohighlight">
\[\text{Noise} = \sigma_\epsilon^2\]</div>
<p>The inherent randomness in the target variable that cannot be explained by features, regardless of model quality.</p>
</section>
<section id="expected-test-error-integrated">
<h2>Expected Test Error (Integrated)<a class="headerlink" href="#expected-test-error-integrated" title="Link to this heading"></a></h2>
<p>Averaging over the entire test set:</p>
<div class="math notranslate nohighlight">
\[\text{Test Error} = \mathbb{E}_{X_0} \left[ \text{EPE}(X_0) \right] = \mathbb{E}_{X_0}[\text{Bias}^2(X_0)] + \mathbb{E}_{X_0}[\text{Var}(X_0)] + \sigma_\epsilon^2\]</div>
<p>This is the <strong>total expected test error</strong> that we aim to minimize.</p>
</section>
<section id="detailed-analysis-of-bias-and-variance">
<h2>Detailed Analysis of Bias and Variance<a class="headerlink" href="#detailed-analysis-of-bias-and-variance" title="Link to this heading"></a></h2>
</section>
<section id="bias-definition-and-interpretation">
<h2>Bias: Definition and Interpretation<a class="headerlink" href="#bias-definition-and-interpretation" title="Link to this heading"></a></h2>
<p><strong>Mathematical Definition:</strong></p>
<div class="math notranslate nohighlight">
\[B(X_0) = f(X_0) - \mathbb{E}_\mathcal{D}[\hat{f}(X_0)]\]</div>
<p><strong>Interpretation:</strong></p>
<ul class="simple">
<li><p>Bias measures how far the <em>average</em> prediction (over all possible training sets) is from the true value</p></li>
<li><p>It reflects the <strong>systematic underfitting</strong> of the model</p></li>
<li><p>Caused by model architecture being too simple to capture true function</p></li>
</ul>
<p><strong>Common Sources of Bias:</strong></p>
<ol class="arabic">
<li><p><strong>Linear models on nonlinear data:</strong></p>
<p>True: <span class="math notranslate nohighlight">\(f(X) = X^2 + \sin(X)\)</span></p>
<p>Model: <span class="math notranslate nohighlight">\(\hat{f}(X) = \beta_0 + \beta_1 X\)</span></p>
<p>The model cannot capture the nonlinearity, creating high bias.</p>
</li>
<li><p><strong>Missing relevant features:</strong></p>
<div class="math notranslate nohighlight">
\[Y = f(X_1, X_2) + \epsilon\]</div>
<p>If we only use <span class="math notranslate nohighlight">\(X_1\)</span>, we introduce omitted variable bias.</p>
</li>
<li><p><strong>Over-regularization:</strong></p>
<p>Heavy regularization (large <span class="math notranslate nohighlight">\(\lambda\)</span> in <span class="math notranslate nohighlight">\(L = \text{MSE} + \lambda \|w\|^2\)</span>) pushes coefficients toward zero, creating bias.</p>
</li>
</ol>
</section>
<section id="variance-definition-and-interpretation">
<h2>Variance: Definition and Interpretation<a class="headerlink" href="#variance-definition-and-interpretation" title="Link to this heading"></a></h2>
<p><strong>Mathematical Definition:</strong></p>
<div class="math notranslate nohighlight">
\[V(X_0) = \mathbb{E}_\mathcal{D}\left[ (\hat{f}(X_0) - \mathbb{E}_\mathcal{D}[\hat{f}(X_0)])^2 \right]\]</div>
<p><strong>Alternative form (law of total variance):</strong></p>
<div class="math notranslate nohighlight">
\[V(X_0) = \mathbb{E}_\mathcal{D}[\hat{f}(X_0)^2] - (\mathbb{E}_\mathcal{D}[\hat{f}(X_0)])^2\]</div>
<p><strong>Interpretation:</strong></p>
<ul class="simple">
<li><p>Variance measures how much the predictions vary across different training sets</p></li>
<li><p>It reflects <strong>overfitting</strong>: model fits training noise rather than true signal</p></li>
<li><p>High variance indicates the model is too sensitive to training data specifics</p></li>
</ul>
<p><strong>Common Sources of Variance:</strong></p>
<ol class="arabic">
<li><p><strong>Complex models with insufficient data:</strong></p>
<p>A 10-degree polynomial fit to 20 data points will vary wildly with small data changes.</p>
</li>
<li><p><strong>Small training set size:</strong></p>
<p>With <span class="math notranslate nohighlight">\(N\)</span> samples, parameter estimates become more unstable.</p>
</li>
<li><p><strong>Under-regularization:</strong></p>
<p>Small <span class="math notranslate nohighlight">\(\lambda\)</span> allows model to fit training noise.</p>
</li>
<li><p><strong>High-dimensional feature spaces:</strong></p>
<p>More parameters to learn with same amount of data → higher variance.</p>
</li>
</ol>
</section>
<section id="relationship-between-bias-and-variance">
<h2>Relationship Between Bias and Variance<a class="headerlink" href="#relationship-between-bias-and-variance" title="Link to this heading"></a></h2>
<p><strong>Key Trade-off:</strong></p>
<ul class="simple">
<li><p><strong>Simple models:</strong> Low variance (stable across datasets), High bias (miss true pattern)</p></li>
<li><p><strong>Complex models:</strong> Low bias (can capture patterns), High variance (sensitive to noise)</p></li>
</ul>
<p><strong>Mathematical insight:</strong></p>
<p>As model complexity increases:</p>
<div class="math notranslate nohighlight">
\[\text{Bias}^2 \text{ decreases (monotonically)}\]</div>
<div class="math notranslate nohighlight">
\[\text{Variance} \text{ increases (monotonically)}\]</div>
<p><strong>Total error:</strong></p>
<div class="math notranslate nohighlight">
\[\text{Total Error} = \text{Bias}^2 + \text{Variance} + \text{Noise}\]</div>
<p>The optimal model complexity minimizes total error, typically at an intermediate complexity level.</p>
</section>
<section id="bias-variance-in-different-contexts">
<h2>Bias-Variance in Different Contexts<a class="headerlink" href="#bias-variance-in-different-contexts" title="Link to this heading"></a></h2>
</section>
<section id="linear-regression">
<h2>Linear Regression<a class="headerlink" href="#linear-regression" title="Link to this heading"></a></h2>
<p>For linear regression <span class="math notranslate nohighlight">\(\hat{f}(X) = X\beta\)</span>, with true model <span class="math notranslate nohighlight">\(Y = f(X) + \epsilon\)</span>:</p>
<p><strong>Bias of OLS estimator:</strong></p>
<div class="math notranslate nohighlight">
\[\text{Bias}(X_0) = X_0(\beta^* - \mathbb{E}[\hat{\beta}])\]</div>
<p>If the true model is <strong>linear</strong> (<span class="math notranslate nohighlight">\(f(X) = X\beta^*\)</span>), OLS is unbiased:</p>
<div class="math notranslate nohighlight">
\[\mathbb{E}[\hat{\beta}] = \beta^*\]</div>
<p>So <span class="math notranslate nohighlight">\(\text{Bias} = 0\)</span> when model is correctly specified.</p>
<p><strong>Variance of OLS estimator:</strong></p>
<div class="math notranslate nohighlight">
\[\text{Var}(\hat{f}(X_0)) = X_0 \text{Cov}(\hat{\beta}) X_0^T = X_0 (\sigma_\epsilon^2 (X^TX)^{-1}) X_0^T\]</div>
<p>Increases with:</p>
<ul class="simple">
<li><p>Noise variance <span class="math notranslate nohighlight">\(\sigma_\epsilon^2\)</span></p></li>
<li><p>High multicollinearity in <span class="math notranslate nohighlight">\(X\)</span> (large <span class="math notranslate nohighlight">\((X^TX)^{-1}\)</span>)</p></li>
<li><p>More features (larger <span class="math notranslate nohighlight">\((X^TX)^{-1}\)</span>)</p></li>
</ul>
</section>
<section id="regularized-regression">
<h2>Regularized Regression<a class="headerlink" href="#regularized-regression" title="Link to this heading"></a></h2>
<p><strong>Ridge Regression</strong> adds penalty: <span class="math notranslate nohighlight">\(\min_\beta \|Y - X\beta\|^2 + \lambda \|\beta\|^2\)</span></p>
<p>The estimator becomes:</p>
<div class="math notranslate nohighlight">
\[\hat{\beta}_{\text{ridge}} = (X^TX + \lambda I)^{-1} X^T Y\]</div>
<p><strong>Bias introduced by regularization:</strong></p>
<div class="math notranslate nohighlight">
\[\text{Bias}(\hat{\beta}_{\text{ridge}}) = -\lambda (X^TX + \lambda I)^{-1} X^T X \beta^*\]</div>
<p>As <span class="math notranslate nohighlight">\(\lambda \to 0\)</span>: Bias <span class="math notranslate nohighlight">\(\to 0\)</span>, but Variance <span class="math notranslate nohighlight">\(\to \infty\)</span></p>
<p>As <span class="math notranslate nohighlight">\(\lambda \to \infty\)</span>: Bias <span class="math notranslate nohighlight">\(\to -X\beta^*\)</span>, but Variance <span class="math notranslate nohighlight">\(\to 0\)</span></p>
<p><strong>Optimal regularization</strong> balances the two.</p>
</section>
<section id="decision-trees-and-ensemble-methods">
<h2>Decision Trees and Ensemble Methods<a class="headerlink" href="#decision-trees-and-ensemble-methods" title="Link to this heading"></a></h2>
<p><strong>Decision Trees:</strong></p>
<ul class="simple">
<li><p><strong>Low bias:</strong> Can approximate any function given enough depth</p></li>
<li><p><strong>High variance:</strong> Small training perturbations cause large tree changes</p></li>
<li><p>Prone to overfitting on noisy data</p></li>
</ul>
<p><strong>Random Forests (Bagging):</strong></p>
<p>Ensemble of <span class="math notranslate nohighlight">\(B\)</span> bootstrap samples, each with tree <span class="math notranslate nohighlight">\(\hat{f}_b(X)\)</span>:</p>
<div class="math notranslate nohighlight">
\[\hat{f}_{\text{bag}}(X) = \frac{1}{B} \sum_{b=1}^B \hat{f}_b(X)\]</div>
<p><strong>Variance reduction:</strong></p>
<div class="math notranslate nohighlight">
\[\text{Var}(\hat{f}_{\text{bag}}) = \frac{\text{Var}(\hat{f})}{B} + \frac{B-1}{B} \text{Cov}(\hat{f}_i, \hat{f}_j)\]</div>
<p>If trees are not perfectly correlated, variance drops significantly.</p>
<p>Bias remains approximately unchanged (still low, from base tree).</p>
<p><strong>Boosting:</strong></p>
<p>Sequentially builds weak learners, reducing both bias and variance:</p>
<div class="math notranslate nohighlight">
\[\hat{f}_{\text{boost}}(X) = \sum_{b=1}^B \alpha_b \hat{f}_b(X)\]</div>
<p>Each iteration focuses on residuals from previous iteration, progressively reducing bias.</p>
<a class="reference internal image-reference" href="../_images/bias_variance_tradeoff.png"><img alt="Bias-Variance tradeoff illustration showing total error, bias squared, and variance" class="align-center" src="../_images/bias_variance_tradeoff.png" style="width: 100%;" />
</a>
</section>
<section id="cross-validation-and-bias-variance">
<h2>Cross-Validation and Bias-Variance<a class="headerlink" href="#cross-validation-and-bias-variance" title="Link to this heading"></a></h2>
</section>
<section id="leave-one-out-cross-validation-loocv">
<h2>Leave-One-Out Cross-Validation (LOOCV)<a class="headerlink" href="#leave-one-out-cross-validation-loocv" title="Link to this heading"></a></h2>
<p>LOOCV error estimates:</p>
<div class="math notranslate nohighlight">
\[\text{CV}_{(n)} = \frac{1}{n} \sum_{i=1}^n (Y_i - \hat{f}^{(-i)}(X_i))^2\]</div>
<p>where <span class="math notranslate nohighlight">\(\hat{f}^{(-i)}\)</span> is trained on all data except observation <span class="math notranslate nohighlight">\(i\)</span>.</p>
<p><strong>Property:</strong> Nearly unbiased estimator of test error (high-variance estimate, though).</p>
</section>
<section id="k-fold-cross-validation">
<h2>K-Fold Cross-Validation<a class="headerlink" href="#k-fold-cross-validation" title="Link to this heading"></a></h2>
<p>Splits data into <span class="math notranslate nohighlight">\(k\)</span> folds:</p>
<div class="math notranslate nohighlight">
\[\text{CV}_{(k)} = \frac{1}{k} \sum_{j=1}^k \text{MSE}_j\]</div>
<p>where <span class="math notranslate nohighlight">\(\text{MSE}_j\)</span> is error on fold <span class="math notranslate nohighlight">\(j\)</span>.</p>
<p><strong>Tradeoff:</strong></p>
<ul class="simple">
<li><p>Larger <span class="math notranslate nohighlight">\(k\)</span> (e.g., <span class="math notranslate nohighlight">\(k=n\)</span>, LOOCV): Lower bias, higher variance</p></li>
<li><p>Smaller <span class="math notranslate nohighlight">\(k\)</span> (e.g., <span class="math notranslate nohighlight">\(k=5\)</span>): Higher bias, lower variance</p></li>
<li><p>Standard choice: <span class="math notranslate nohighlight">\(k=5\)</span> or <span class="math notranslate nohighlight">\(k=10\)</span> balances both</p></li>
</ul>
<a class="reference internal image-reference" href="../_images/cross_validation_bias_variance.png"><img alt="Cross-validation error curves and quality metrics" class="align-center" src="../_images/cross_validation_bias_variance.png" style="width: 100%;" />
</a>
</section>
<section id="learning-curves-diagnosing-bias-variance-problems">
<h2>Learning Curves: Diagnosing Bias-Variance Problems<a class="headerlink" href="#learning-curves-diagnosing-bias-variance-problems" title="Link to this heading"></a></h2>
<p>Learning curves plot training and validation error versus training set size <span class="math notranslate nohighlight">\(n\)</span>.</p>
<p><strong>Underfitting (High Bias):</strong></p>
<div class="math notranslate nohighlight">
\[\lim_{n \to \infty} \text{Train Error} = \lim_{n \to \infty} \text{Val Error} = \text{High}\]</div>
<p>Both curves plateau at a high error level, with small gap between them.</p>
<p><strong>Overfitting (High Variance):</strong></p>
<div class="math notranslate nohighlight">
\[\text{Train Error} \ll \text{Val Error} \text{ for all } n\]</div>
<p>Large gap persists even with more data; training error stays low while validation error remains high.</p>
<p><strong>Good fit:</strong></p>
<div class="math notranslate nohighlight">
\[\text{Train Error} \approx \text{Val Error} \approx \text{Bayes Error}\]</div>
<p>Both curves converge with manageable gap.</p>
<a class="reference internal image-reference" href="../_images/learning_curves_bias_variance.png"><img alt="Learning curves showing underfitting, good fit, and overfitting scenarios" class="align-center" src="../_images/learning_curves_bias_variance.png" style="width: 100%;" />
</a>
</section>
<section id="model-complexity-examples">
<h2>Model Complexity Examples<a class="headerlink" href="#model-complexity-examples" title="Link to this heading"></a></h2>
<p>Below are examples of fitted models with varying polynomial degrees showing the bias-variance tradeoff:</p>
<a class="reference internal image-reference" href="../_images/model_complexity_effect.png"><img alt="Fitted polynomial models of varying degrees demonstrating bias-variance" class="align-center" src="../_images/model_complexity_effect.png" style="width: 100%;" />
</a>
</section>
<section id="practical-strategies-to-control-bias-variance">
<h2>Practical Strategies to Control Bias-Variance<a class="headerlink" href="#practical-strategies-to-control-bias-variance" title="Link to this heading"></a></h2>
</section>
<section id="reducing-bias-model-is-too-simple">
<h2>Reducing Bias (Model is Too Simple)<a class="headerlink" href="#reducing-bias-model-is-too-simple" title="Link to this heading"></a></h2>
<ol class="arabic simple">
<li><p><strong>Increase model complexity:</strong></p>
<ul class="simple">
<li><p>Use higher-degree polynomials</p></li>
<li><p>Add more features (with caution)</p></li>
<li><p>Use nonlinear models (neural networks, kernels)</p></li>
</ul>
</li>
<li><p><strong>Reduce regularization:</strong></p>
<ul class="simple">
<li><p>Decrease <span class="math notranslate nohighlight">\(\lambda\)</span> in ridge/LASSO</p></li>
<li><p>Reduce tree depth constraints</p></li>
<li><p>Increase ensemble size</p></li>
</ul>
</li>
<li><p><strong>Feature engineering:</strong></p>
<ul class="simple">
<li><p>Add interaction terms</p></li>
<li><p>Add polynomial features</p></li>
<li><p>Domain-specific transformations</p></li>
</ul>
</li>
</ol>
<p><strong>Example:</strong> If polynomial regression underfits, increase degree from 2 to 3 or 5.</p>
</section>
<section id="reducing-variance-model-is-too-complex">
<h2>Reducing Variance (Model is Too Complex)<a class="headerlink" href="#reducing-variance-model-is-too-complex" title="Link to this heading"></a></h2>
<ol class="arabic">
<li><p><strong>Decrease model complexity:</strong></p>
<ul class="simple">
<li><p>Use lower-degree polynomials</p></li>
<li><p>Reduce number of features (feature selection)</p></li>
<li><p>Limit tree depth, require minimum samples per leaf</p></li>
</ul>
</li>
<li><p><strong>Increase regularization:</strong></p>
<ul class="simple">
<li><p>Increase <span class="math notranslate nohighlight">\(\lambda\)</span> in ridge/LASSO</p></li>
<li><p>Add early stopping in boosting/neural networks</p></li>
<li><p>Use dropout in neural networks</p></li>
</ul>
</li>
<li><p><strong>Get more training data:</strong></p>
<div class="math notranslate nohighlight">
\[\text{Var} \propto \frac{\sigma_\epsilon^2}{n}\]</div>
<p>Increasing <span class="math notranslate nohighlight">\(n\)</span> reduces variance directly.</p>
</li>
<li><p><strong>Ensemble methods:</strong></p>
<ul class="simple">
<li><p>Bagging/Random Forests reduce variance without increasing bias</p></li>
<li><p>Helps decorrelate predictions across bootstrap samples</p></li>
</ul>
</li>
<li><p><strong>Hyperparameter tuning:</strong></p>
<p>Use cross-validation to find <span class="math notranslate nohighlight">\(\lambda\)</span>, <span class="math notranslate nohighlight">\(k\)</span>, etc. that minimize validation error.</p>
</li>
</ol>
<a class="reference internal image-reference" href="../_images/bias_variance_reduction_strategies.png"><img alt="Strategies for reducing bias and variance in models" class="align-center" src="../_images/bias_variance_reduction_strategies.png" style="width: 100%;" />
</a>
</section>
<section id="the-bayes-error-ultimate-lower-bound">
<h2>The Bayes Error: Ultimate Lower Bound<a class="headerlink" href="#the-bayes-error-ultimate-lower-bound" title="Link to this heading"></a></h2>
<p><strong>Bayes error</strong> (also <strong>irreducible error</strong>):</p>
<div class="math notranslate nohighlight">
\[\epsilon_{\text{Bayes}} = \inf_{\hat{f}} \mathbb{E}[(Y - \hat{f}(X))^2]\]</div>
<p>This is the best possible error using <em>any</em> model, achieved when <span class="math notranslate nohighlight">\(\hat{f}(X) = \mathbb{E}[Y|X]\)</span>.</p>
<p><strong>Total error decomposition:</strong></p>
<div class="math notranslate nohighlight">
\[\text{Total Error} = \underbrace{\text{Bias}^2 + \text{Variance}}_{\text{Reducible Error}} + \underbrace{\epsilon_{\text{Bayes}}}_{\text{Irreducible Error}}\]</div>
<p>In noisy problems (<span class="math notranslate nohighlight">\(\sigma_\epsilon^2\)</span> large), even perfect models incur significant error.</p>
<p><strong>Practical consequence:</strong></p>
<ul class="simple">
<li><p>Don’t obsess over achieving zero training error if noise is high</p></li>
<li><p>Target validation error near Bayes error + small margin for model imperfection</p></li>
</ul>
</section>
<section id="bias-variance-in-classification">
<h2>Bias-Variance in Classification<a class="headerlink" href="#bias-variance-in-classification" title="Link to this heading"></a></h2>
<p>For classification with 0-1 loss:</p>
<div class="math notranslate nohighlight">
\[L(Y, \hat{f}(X)) = \mathbb{1}[Y \neq \hat{f}(X)]\]</div>
<p><strong>Misclassification error:</strong></p>
<div class="math notranslate nohighlight">
\[\text{Err}(X_0) = \Pr(\hat{f}(X_0) \neq f(X_0))\]</div>
<p>Bias-variance decomposition still applies but is more nuanced:</p>
<div class="math notranslate nohighlight">
\[\text{Bias} = \Pr(f(X_0) \neq \mathbb{E}[\hat{f}(X_0)])\]</div>
<p>Models with high variance may still have <strong>low bias</strong> in classification if variance is around the correct decision boundary.</p>
<p><strong>Example:</strong> A 3-nearest neighbor classifier has higher variance but potentially lower bias than 1-nearest neighbor.</p>
<a class="reference internal image-reference" href="../_images/bias_variance_classification.png"><img alt="Bias-variance tradeoff in classification with decision boundaries" class="align-center" src="../_images/bias_variance_classification.png" style="width: 100%;" />
</a>
</section>
<section id="summary-table">
<h2>Summary Table<a class="headerlink" href="#summary-table" title="Link to this heading"></a></h2>
<table class="docutils align-default" id="id1">
<caption><span class="caption-text">Bias-Variance Characteristics</span><a class="headerlink" href="#id1" title="Link to this table"></a></caption>
<colgroup>
<col style="width: 20.0%" />
<col style="width: 25.0%" />
<col style="width: 25.0%" />
<col style="width: 30.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Aspect</p></th>
<th class="head"><p><strong>High Bias</strong></p></th>
<th class="head"><p><strong>High Variance</strong></p></th>
<th class="head"><p><strong>Balanced</strong></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Model Type</p></td>
<td><p>Too simple (linear on nonlinear)</p></td>
<td><p>Too complex (overfitting)</p></td>
<td><p>Appropriate complexity</p></td>
</tr>
<tr class="row-odd"><td><p>Training Error</p></td>
<td><p>High</p></td>
<td><p>Low</p></td>
<td><p>Moderate</p></td>
</tr>
<tr class="row-even"><td><p>Validation Error</p></td>
<td><p>High</p></td>
<td><p>High</p></td>
<td><p>Low</p></td>
</tr>
<tr class="row-odd"><td><p>Train-Val Gap</p></td>
<td><p>Small</p></td>
<td><p>Large</p></td>
<td><p>Small</p></td>
</tr>
<tr class="row-even"><td><p>Cause</p></td>
<td><p>Underfitting</p></td>
<td><p>Overfitting</p></td>
<td><p>Good fit</p></td>
</tr>
<tr class="row-odd"><td><p>Cure</p></td>
<td><p>Add complexity, features, reduce <span class="math notranslate nohighlight">\(\lambda\)</span></p></td>
<td><p>Reduce complexity, increase <span class="math notranslate nohighlight">\(\lambda\)</span>, get data</p></td>
<td><p>Fine-tune hyperparameters</p></td>
</tr>
<tr class="row-even"><td><p>Learning Curve</p></td>
<td><p>Both curves high, converge</p></td>
<td><p>Large gap, gap persists</p></td>
<td><p>Small gap, both converge to low error</p></td>
</tr>
</tbody>
</table>
</section>
<section id="references-and-further-reading">
<h2>References and Further Reading<a class="headerlink" href="#references-and-further-reading" title="Link to this heading"></a></h2>
<ul class="simple">
<li><p><strong>Geman, S., Bienenstock, E., &amp; Doursat, R.</strong> (1992). Neural networks and the bias/variance dilemma. <em>Neural Computation</em>, 4(1), 1–58. Classic paper formalizing the tradeoff for neural nets.</p></li>
<li><p><strong>Domingos, P.</strong> (2000). A unified bias-variance decomposition. In <em>ICML</em>. Extends the decomposition to many loss functions beyond squared error.</p></li>
<li><p><strong>Hastie, T., Tibshirani, R., &amp; Friedman, J.</strong> (2009). <em>The Elements of Statistical Learning</em> (2nd ed.). Springer. Chapter 7 provides practical guidance on bias-variance and model selection.</p></li>
<li><p><strong>Bishop, C. M.</strong> (2006). <em>Pattern Recognition and Machine Learning</em>. Springer. Sections 1.3 and 3.2 connect bias-variance with Bayesian viewpoints.</p></li>
<li><p><strong>Kuhn, M., &amp; Johnson, K.</strong> (2013). <em>Applied Predictive Modeling</em>. Springer. Chapter 4 covers bias-variance diagnostics with real-world examples.</p></li>
</ul>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="logistic_regression.html" class="btn btn-neutral float-left" title="Logistic Regression" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="metrics.html" class="btn btn-neutral float-right" title="Metrics for DS, ML, and AI" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright J. V. V. Cassiano.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>